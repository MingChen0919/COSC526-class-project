{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SQLContext\n",
    "# \n",
    "# # create spark contexts\n",
    "# sc = pyspark.SparkContext()\n",
    "# sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neccessary modules and python files\n",
    "* **langid**\n",
    "* **nltk**\n",
    "\n",
    "### Install **langid**\n",
    "\n",
    "```\n",
    "pip install langid\n",
    "```\n",
    "\n",
    "### Download nltk\n",
    "\n",
    "* install **nltk** module\n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "* download corpora\n",
    "```\n",
    "# enter python interactive environment\n",
    "python\n",
    "# type python script\n",
    "from ntlk import download\n",
    "download()\n",
    "```\n",
    "\n",
    "\n",
    "### Get `preproc.py`\n",
    "* Reference: https://github.com/dreyco676/nlp_spark\n",
    "* Get `preproc.py`: `wget https://raw.githubusercontent.com/dreyco676/nlp_spark/master/preproc.py`\n",
    "* `preproc.py` has to be in the same directory with your *ipynb* file\n",
    "\n",
    "### Get practice data\n",
    "\n",
    "```\n",
    "git clone https://github.com/dreyco676/nlp_spark.git\n",
    "cd nlp_spark/\n",
    "unzip data.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "# import preproc as pp\n",
    "# # Register all the functions in Preproc with Spark Context\n",
    "# check_lang_udf = udf(pp.check_lang, StringType())\n",
    "# remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "# remove_features_udf = udf(pp.remove_features, StringType())\n",
    "# tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "# lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "# check_blanks_udf = udf(pp.check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_classified = spark.read.csv('nlp_spark/data/raw_classified.txt', inferSchema=True, sep='\\t').toDF('text', 'id', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+\n",
      "|                text|                id|label|\n",
      "+--------------------+------------------+-----+\n",
      "|Fresh install of ...|        1018769417|  1.0|\n",
      "|Well. Now I know ...|       10284216536|  1.0|\n",
      "|\"Literally six we...|       10298589026|  1.0|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|\n",
      "|'Cheap Eats in SL...|109642968603963392|  1.0|\n",
      "+--------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove single/double quotes and space at the begining and end of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "trim_quotes_and_space_udf = udf(lambda s: s.strip().strip('\"').strip(\"'\"), StringType())\n",
    "raw_classified_v1 = raw_classified.withColumn('text', trim_quotes_and_space_udf(raw_classified.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+\n",
      "|                text|                id|label|\n",
      "+--------------------+------------------+-----+\n",
      "|Fresh install of ...|        1018769417|  1.0|\n",
      "|Well. Now I know ...|       10284216536|  1.0|\n",
      "|Literally six wee...|       10298589026|  1.0|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|\n",
      "|Cheap Eats in SLP...|109642968603963392|  1.0|\n",
      "+--------------------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows: 115886\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v1.show(5)\n",
    "print('Total rows: {}'.format(raw_classified_v1.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingchen/anaconda2/lib/python2.7/site-packages/pytz/__init__.py:29: UserWarning: Module argparse was already imported from /Users/mingchen/anaconda2/lib/python2.7/argparse.pyc, but /Users/mingchen/anaconda2/lib/python2.7/site-packages/argparse-1.4.0-py2.7.egg is being added to sys.path\n",
      "  from pkg_resources import resource_stream\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u\"Cheap Eats in SLP' - http://t.co/4w8gRp7\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_classified_v1.select('text').toPandas()['text'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check string length in column 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "string_length_udf = udf(lambda s: len(s), IntegerType())\n",
    "raw_classified_v2 = raw_classified_v1.withColumn('text_length', string_length_udf(raw_classified_v1.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----+-----------+\n",
      "|      text|                id|label|text_length|\n",
      "+----------+------------------+-----+-----------+\n",
      "|   awesome|         882098800|  1.0|          7|\n",
      "|  City Wok|665255511273570305|  1.0|          8|\n",
      "| #iknowhow|537734337655889921|  1.0|          9|\n",
      "|It's cold.|159983261911760896|  1.0|         10|\n",
      "|Boarded :)|413084478760685568|  1.0|         10|\n",
      "+----------+------------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+------------------+-----+-----------+\n",
      "|                text|                id|label|text_length|\n",
      "+--------------------+------------------+-----+-----------+\n",
      "|A girlfriend that...|424763466897846272|  1.0|        256|\n",
      "|Real boyfriends &...|429215572241285121|  1.0|        166|\n",
      "|RT @julieklausner...|199565254542376960|  1.0|        164|\n",
      "|Real boyfriends &...|418449856298889216|  1.0|        164|\n",
      "|Damn. I want to s...|445063125696401409|  1.0|        159|\n",
      "+--------------------+------------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v2.orderBy('text_length').show(5)\n",
    "raw_classified_v2.orderBy('text_length', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The minimum length is 7 and maximum length is 256. Therefore, no empty strings or None values in column 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+-----------+\n",
      "|                text|                id|label|text_length|\n",
      "+--------------------+------------------+-----+-----------+\n",
      "|Fresh install of ...|        1018769417|  1.0|         61|\n",
      "|Well. Now I know ...|       10284216536|  1.0|         85|\n",
      "|Literally six wee...|       10298589026|  1.0|        134|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|         90|\n",
      "|Cheap Eats in SLP...|109642968603963392|  1.0|         40|\n",
      "+--------------------+------------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many labels are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  1.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified.select('label').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check values in column 'id'\n",
    "    + From the id sorted results, no NA, None values exist in column 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+-----------+\n",
      "|                text|       id|label|text_length|\n",
      "+--------------------+---------+-----+-----------+\n",
      "|Sorry! Account de...|797858706|  1.0|         59|\n",
      "|Yo am I imagining...|798243247|  1.0|        127|\n",
      "|Midnight coffee i...|798474877|  1.0|         48|\n",
      "|I'm sad that Mike...|799151574|  1.0|         45|\n",
      "|      Peter fixed it|799331338|  1.0|         14|\n",
      "+--------------------+---------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+------------------+-----+-----------+\n",
      "|                text|                id|label|text_length|\n",
      "+--------------------+------------------+-----+-----------+\n",
      "|White Dwarf 100 \"...|679856481798369282|  1.0|        123|\n",
      "|White Dwarf 100 \"...|679856481798369282|  1.0|        123|\n",
      "|White Dwarf 100 �...|679851755815985153|  1.0|        135|\n",
      "|RT @iwan0www: Pro...|679847620995579904|  1.0|        122|\n",
      "|RT @mikeolson: 9 ...|679847263238250497|  0.0|        140|\n",
      "+--------------------+------------------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v2.orderBy('id').show(5)\n",
    "raw_classified_v2.orderBy('id', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from langid import classify\n",
    "identify_lang_code_udf = udf(lambda s: classify(s)[0])\n",
    "identify_lang_prob_udf = udf(lambda s: classify(s)[1])\n",
    "raw_classified_v3 = raw_classified_v2.\\\n",
    "                    withColumn('lang', identify_lang_code_udf(raw_classified_v2.text)).\\\n",
    "                    withColumn('lang_prob', identify_lang_prob_udf(raw_classified_v3.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+-----------+----+-------------------+\n",
      "|                text|                id|label|text_length|lang|          lang_prob|\n",
      "+--------------------+------------------+-----+-----------+----+-------------------+\n",
      "|Fresh install of ...|        1018769417|  1.0|         61|  en| -29.55938959121704|\n",
      "|Well. Now I know ...|       10284216536|  1.0|         85|  en|-159.12265014648438|\n",
      "|Literally six wee...|       10298589026|  1.0|        134|  en| -183.2326889038086|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|         90|  en|-108.64056301116943|\n",
      "|Cheap Eats in SLP...|109642968603963392|  1.0|         40|  en|-16.089602947235107|\n",
      "+--------------------+------------------+-----+-----------+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|           lang_prob|\n",
      "+-------+--------------------+\n",
      "|  count|              115886|\n",
      "|   mean| -138.90801413837985|\n",
      "| stddev|   78.35839492856009|\n",
      "|    min|-0.05613517761230469|\n",
      "|    max|  3.6121368408203125|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v3.describe('lang_prob').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+-----------+----+--------------------+\n",
      "|                text|                id|label|text_length|lang|           lang_prob|\n",
      "+--------------------+------------------+-----+-----------+----+--------------------+\n",
      "|Automating big-da...|654986512367398915|  0.0|         74|  en|-0.05613517761230469|\n",
      "|2010 Home Remodel...|       12962035649|  1.0|         66|  en| -0.5808463096618652|\n",
      "|Men Explain &lt;e...|678837202969751552|  1.0|         84|  en| -0.9656310081481934|\n",
      "|Google Analytics ...|513002286629806080|  1.0|         85|  en| -0.9861445426940918|\n",
      "|RT @albertpak: #E...|659205951313022977|  1.0|         93|  en| -1.0300307273864746|\n",
      "+--------------------+------------------+-----+-----------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v3.orderBy('lang_prob').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text_length</th>\n",
       "      <th>lang</th>\n",
       "      <th>lang_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Automating big-data analysis http://t.co/oKmeA...</td>\n",
       "      <td>654986512367398915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>en</td>\n",
       "      <td>-0.05613517761230469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010 Home Remodeling Tour in St. Louis Park - ...</td>\n",
       "      <td>12962035649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>66</td>\n",
       "      <td>en</td>\n",
       "      <td>-0.5808463096618652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Men Explain &amp;lt;em&amp;gt;Lolita &amp;lt;/em&amp;gt;to Me ...</td>\n",
       "      <td>678837202969751552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84</td>\n",
       "      <td>en</td>\n",
       "      <td>-0.9656310081481934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google Analytics Adds AdWords Return on Ad Spe...</td>\n",
       "      <td>513002286629806080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85</td>\n",
       "      <td>en</td>\n",
       "      <td>-0.9861445426940918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @albertpak: #ES6 Overview in 350 Bullet Poi...</td>\n",
       "      <td>659205951313022977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93</td>\n",
       "      <td>en</td>\n",
       "      <td>-1.0300307273864746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @blattnerma: Modern Methods for Sentiment A...</td>\n",
       "      <td>583126282373865472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93</td>\n",
       "      <td>en</td>\n",
       "      <td>-1.1199603080749512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#TipOfTheDay Indexing in #SQL http://t.co/XM9K...</td>\n",
       "      <td>622269821862981632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52</td>\n",
       "      <td>en</td>\n",
       "      <td>-1.2104015350341797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BBC News: Donut-shaped 'compass' in fly brain ...</td>\n",
       "      <td>598903110258524160</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70</td>\n",
       "      <td>en</td>\n",
       "      <td>-1.233856201171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tell me a story!</td>\n",
       "      <td>25346982865</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>en</td>\n",
       "      <td>-1.2732758522033691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I love his smile.</td>\n",
       "      <td>426575800360394752</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>-1.4227490425109863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  id  \\\n",
       "0  Automating big-data analysis http://t.co/oKmeA...  654986512367398915   \n",
       "1  2010 Home Remodeling Tour in St. Louis Park - ...         12962035649   \n",
       "2  Men Explain &lt;em&gt;Lolita &lt;/em&gt;to Me ...  678837202969751552   \n",
       "3  Google Analytics Adds AdWords Return on Ad Spe...  513002286629806080   \n",
       "4  RT @albertpak: #ES6 Overview in 350 Bullet Poi...  659205951313022977   \n",
       "5  RT @blattnerma: Modern Methods for Sentiment A...  583126282373865472   \n",
       "6  #TipOfTheDay Indexing in #SQL http://t.co/XM9K...  622269821862981632   \n",
       "7  BBC News: Donut-shaped 'compass' in fly brain ...  598903110258524160   \n",
       "8                                   Tell me a story!         25346982865   \n",
       "9                                  I love his smile.  426575800360394752   \n",
       "\n",
       "   label  text_length lang             lang_prob  \n",
       "0    0.0           74   en  -0.05613517761230469  \n",
       "1    1.0           66   en   -0.5808463096618652  \n",
       "2    1.0           84   en   -0.9656310081481934  \n",
       "3    1.0           85   en   -0.9861445426940918  \n",
       "4    1.0           93   en   -1.0300307273864746  \n",
       "5    0.0           93   en   -1.1199603080749512  \n",
       "6    1.0           52   en   -1.2104015350341797  \n",
       "7    1.0           70   en    -1.233856201171875  \n",
       "8    1.0           16   en   -1.2732758522033691  \n",
       "9    1.0           17   en   -1.4227490425109863  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_classified_v3.orderBy('lang_prob').toPandas()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* select 'english' rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115886"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_classified_v4 = raw_classified_v3.filter(raw_classified_v3.lang == 'en')\n",
    "raw_classified_v4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Remove stopwords  to reduce dimensionality\n",
    "\n",
    "    * reference: http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    stopwords_arr = stopwords.words('English')\n",
    "    words_arr = s.split()\n",
    "    cleaned_words = []\n",
    "    for word in words_arr:\n",
    "        ## lowercase the word\n",
    "        if word.lower() not in stopwords_arr:\n",
    "            cleaned_words.append(word)\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "remove_stopwords_udf = udf(remove_stopwords, StringType())\n",
    "\n",
    "from preproc import remove_features\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "\n",
    "raw_classified_v5 = raw_classified_v4.\\\n",
    "                    withColumn('no_stopwords_text', remove_stopwords_udf(raw_classified_v4.text))\n",
    "raw_classified_v6 = raw_classified_v5.\\\n",
    "                    withColumn('no_features_text', remove_features_udf(raw_classified_v5.no_stopwords_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+----+-------------------+--------------------+\n",
      "|                text|                id|label|lang|          lang_prob|    no_features_text|\n",
      "+--------------------+------------------+-----+----+-------------------+--------------------+\n",
      "|Fresh install of ...|        1018769417|  1.0|  en| -29.55938959121704|fresh install  ne...|\n",
      "|Well. Now I know ...|       10284216536|  1.0|  en|-159.12265014648438|well know  want k...|\n",
      "|Literally six wee...|       10298589026|  1.0|  en| -183.2326889038086|literally six wee...|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|  en|-108.64056301116943|mitsubishi miev w...|\n",
      "|Cheap Eats in SLP...|109642968603963392|  1.0|  en|-16.089602947235107|      cheap eats slp|\n",
      "|Teenage Mutant Ni...|       10995492579|  1.0|  en| -97.68339467048645|teenage mutant ni...|\n",
      "|New demographic s...|       11713360136|  1.0|  en| -67.72868013381958|new demographic s...|\n",
      "|hi all - i'm goin...|        1208319583|  1.0|  en|-223.62554097175598|    going tweetin...|\n",
      "|Holy carp, no. Th...|121330835726155776|  1.0|  en|-189.64943313598633|holy carp  movie ...|\n",
      "|Did I really need...|       12358025545|  1.0|  en|-260.65723061561584|really need learn...|\n",
      "|http://bit.ly/cNN...|       12463427221|  1.0|  en|-162.11929559707642|basically screwed...|\n",
      "|Great story about...|129187584286003200|  1.0|  en|-155.20797395706177|great story cafe ...|\n",
      "|#SPACC member mtg...|134773454061834240|  1.0|  en|-115.37513542175293|spacc member mtg ...|\n",
      "|Wow it's already ...|       13529858328|  1.0|  en| -8.641799449920654|wow   already mid...|\n",
      "|Wow it's already ...|       13529858328|  1.0|  en| -8.641799449920654|wow   already mid...|\n",
      "|Welcome to Shadra...|       13928357190|  1.0|  en|-143.32168054580688|welcome shadrack ...|\n",
      "|any presentation ...|141226679912759296|  1.0|  en|-197.01211977005005|presentation get ...|\n",
      "|You can know the ...| 14852201499328512|  1.0|  en|-388.93899965286255|know name bird la...|\n",
      "|RT @scatx: This c...|148661624063012864|  1.0|  en|  -68.7083306312561|  comment beautif...|\n",
      "|RT @SGgrc: HP 15C...|149586936359686145|  1.0|  en| -82.53282928466797|    calculator li...|\n",
      "+--------------------+------------------+-----+----+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v7 = raw_classified_v6.select('text','id','label','lang','lang_prob','no_features_text')\n",
    "raw_classified_v7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag and keep verbs, nouns, adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preproc import tag_and_remove\n",
    "tag_and_remove_udf = udf(tag_and_remove, StringType())\n",
    "raw_classified_v8 = raw_classified_v7.\\\n",
    "                    withColumn('tagged_text', remove_features_udf(raw_classified_v7.no_features_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+\n",
      "|                text|                id|label|lang|          lang_prob|    no_features_text|         tagged_text|\n",
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+\n",
      "|Fresh install of ...|        1018769417|  1.0|  en| -29.55938959121704|fresh install  ne...|fresh install new...|\n",
      "|Well. Now I know ...|       10284216536|  1.0|  en|-159.12265014648438|well know  want k...|well know want kn...|\n",
      "|Literally six wee...|       10298589026|  1.0|  en| -183.2326889038086|literally six wee...|literally six wee...|\n",
      "|Mitsubishi i MiEV...|109017669432377344|  1.0|  en|-108.64056301116943|mitsubishi miev w...|mitsubishi miev w...|\n",
      "|Cheap Eats in SLP...|109642968603963392|  1.0|  en|-16.089602947235107|      cheap eats slp|      cheap eats slp|\n",
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_classified_v8.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmitizing remaining words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preproc import lemmatize\n",
    "lemmatize_udf = udf(lemmatize, StringType())\n",
    "lemm_df = raw_classified_v8 = raw_classified_v8.\\\n",
    "                              withColumn('lemm_text', remove_features_udf(raw_classified_v8.tagged_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+--------------------+\n",
      "|                text|                id|label|lang|          lang_prob|    no_features_text|         tagged_text|           lemm_text|\n",
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+--------------------+\n",
      "|KS is up! http://...|335930628354752512|  1.0|  en|  -31.7122220993042|                    |                    |                    |\n",
      "|RT @AAA_Minneapol...|561370893637545987|  1.0|  en| -156.6396360397339|  aaa minneapolis...|aaa minneapolis k...|aaa minneapolis k...|\n",
      "|Aaaa, don't ask m...|106045997377003521|  1.0|  en|-200.86466455459595|aaaa don  ask rea...|aaaa don ask read...|aaaa don ask read...|\n",
      "|AAAAAAAAAAAAA Kin...|        2365823927|  1.0|  en|-25.333629608154297|aaaaaaaaaaaaa kin...|aaaaaaaaaaaaa kin...|aaaaaaaaaaaaa kin...|\n",
      "|RT @Mike_Doughty_...|356241623140671490|  1.0|  en| -67.22585916519165|   aaaaaaand context|   aaaaaaand context|   aaaaaaand context|\n",
      "|Aaaaaaaugh. I fel...|        1556831247|  1.0|  en|-201.82413005828857|aaaaaaaugh fell b...|aaaaaaaugh fell b...|aaaaaaaugh fell b...|\n",
      "|RT @christinacaci...|540598478418608128|  1.0|  en| -257.1230628490448|  aaaaaalllll thi...|aaaaaalllll thing...|aaaaaalllll thing...|\n",
      "|Aaaaaand done.  h...|       10752482526|  1.0|  en|-24.887291431427002|       aaaaaand done|       aaaaaand done|       aaaaaand done|\n",
      "|Aaaaaand that's i...|506636444874838016|  1.0|  en|-164.48618245124817|aaaaaand that   t...|aaaaaand that tha...|aaaaaand that tha...|\n",
      "|RT @NicolaiGrunne...|609163535113940993|  1.0|  en| -220.5425295829773|  aaaaand another...|aaaaand another c...|aaaaand another c...|\n",
      "|RT @opencalais: A...|529429040906899456|  1.0|  en|-106.10052061080933|  aaaaand   back ...|aaaaand back come...|aaaaand back come...|\n",
      "|Aaaaand I'm still...|651644200044265472|  1.0|  en| -85.61156129837036|aaaaand   still w...|aaaaand still wor...|aaaaand still wor...|\n",
      "|Aaaaand I'm still...|651644200044265472|  1.0|  en| -85.61156129837036|aaaaand   still w...|aaaaand still wor...|aaaaand still wor...|\n",
      "|Aaaaand wiped off...|404086393711755264|  1.0|  en| -67.11825656890869|   aaaaand wiped map|   aaaaand wiped map|   aaaaand wiped map|\n",
      "|Aaaand here comes...|550863884622778368|  1.0|  en| -85.47705268859863|aaaand comes chil...|aaaand comes chil...|aaaand comes chil...|\n",
      "|Aaaand just like ...|574469240058744832|  1.0|  en| -134.9506242275238|aaaand like that ...|aaaand like that ...|aaaand like that ...|\n",
      "|RT @lindevi: Aaaa...|479674545171730432|  1.0|  en| -87.05994606018066|  aaaand steam su...|aaaand steam summ...|aaaand steam summ...|\n",
      "|RT @lowqfah: @pri...|600370379689959424|  1.0|  en| -75.18309593200684|  aaaawooooo were...|aaaawooooo werewo...|aaaawooooo werewo...|\n",
      "|RT @Tilaurin: Aaa...|593580103935893504|  1.0|  en| -89.77088737487793|  aaand goreshade...|aaand goreshade d...|aaand goreshade d...|\n",
      "|Aaand old documen...|538437282219692032|  1.0|  en|  -223.785076379776|aaand old documen...|aaand old documen...|aaand old documen...|\n",
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemm_df.orderBy('tagged_text').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove all rows containing only blank spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preproc import check_blanks\n",
    "check_blanks_udf = udf(check_blanks, StringType())\n",
    "\n",
    "check_blanks_df = lemm_df.withColumn(\"is_blank\", check_blanks_udf(lemm_df[\"lemm_text\"]))\n",
    "no_blanks_df = check_blanks_df.filter(check_blanks_df[\"is_blank\"] == \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+--------------------+--------+\n",
      "|                text|                id|label|lang|          lang_prob|    no_features_text|         tagged_text|           lemm_text|is_blank|\n",
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+--------------------+--------+\n",
      "|fresh install new...|        1018769417|  1.0|  en| -29.55938959121704|fresh install  ne...|fresh install new...|fresh install new...|   False|\n",
      "|well know want kn...|       10284216536|  1.0|  en|-159.12265014648438|well know  want k...|well know want kn...|well know want kn...|   False|\n",
      "|literally six wee...|       10298589026|  1.0|  en| -183.2326889038086|literally six wee...|literally six wee...|literally six wee...|   False|\n",
      "|mitsubishi miev w...|109017669432377344|  1.0|  en|-108.64056301116943|mitsubishi miev w...|mitsubishi miev w...|mitsubishi miev w...|   False|\n",
      "|      cheap eats slp|109642968603963392|  1.0|  en|-16.089602947235107|      cheap eats slp|      cheap eats slp|      cheap eats slp|   False|\n",
      "+--------------------+------------------+-----+----+-------------------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_blanks_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "no_blanks_df = no_blanks_df.withColumn(\"text\",no_blanks_df.lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dedupe important since alot of the tweets only differed by url's and RT mentions\n",
    "dedup_df = no_blanks_df.dropDuplicates(['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select only the columns we care about\n",
    "data_set = dedup_df.select('id', 'text','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+\n",
      "|                id|                text|label|\n",
      "+------------------+--------------------+-----+\n",
      "|563146734163865600|annual dinner tha...|  1.0|\n",
      "|163934632260288512|new blog post arm...|  1.0|\n",
      "|416823132796637184|help what area pa...|  1.0|\n",
      "|615340297178992640|much panic would ...|  1.0|\n",
      "+------------------+--------------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data_set.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier \n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sting tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+--------------------+\n",
      "|                id|                text|label|              tokens|\n",
      "+------------------+--------------------+-----+--------------------+\n",
      "|563146734163865600|annual dinner tha...|  1.0|[annual, dinner, ...|\n",
      "|163934632260288512|new blog post arm...|  1.0|[new, blog, post,...|\n",
      "|416823132796637184|help what area pa...|  1.0|[help, what, area...|\n",
      "|615340297178992640|much panic would ...|  1.0|[much, panic, wou...|\n",
      "|679049430213988352|improve dataliter...|  1.0|[improve, datalit...|\n",
      "+------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer_mod = Tokenizer(inputCol='text', outputCol='tokens')\n",
    "tokenizer_df = tokenizer_mod.transform(data_set)\n",
    "tokenizer_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing token frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+--------------------+--------------------+\n",
      "|                id|                text|label|              tokens|            features|\n",
      "+------------------+--------------------+-----+--------------------+--------------------+\n",
      "|563146734163865600|annual dinner tha...|  1.0|[annual, dinner, ...|(262144,[12336,24...|\n",
      "|163934632260288512|new blog post arm...|  1.0|[new, blog, post,...|(262144,[20785,29...|\n",
      "|416823132796637184|help what area pa...|  1.0|[help, what, area...|(262144,[2025,880...|\n",
      "|615340297178992640|much panic would ...|  1.0|[much, panic, wou...|(262144,[11910,29...|\n",
      "|679049430213988352|improve dataliter...|  1.0|[improve, datalit...|(262144,[48482,99...|\n",
      "+------------------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF_mod = HashingTF(inputCol='tokens', outputCol='features')\n",
    "hashingTF_df = hashingTF_mod.transform(tokenizer_df)\n",
    "hashingTF_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(262144, {12336: 1.0, 24113: 1.0, 124634: 1.0, 132458: 1.0, 149413: 1.0, 156567: 1.0, 192114: 1.0, 194253: 1.0, 216199: 1.0, 251358: 1.0}))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashingTF_df.select('features').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "\n",
    "# A tokenizer that converts the input string to lowercase and then splits it by white spaces.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"features\", outputCol=\"idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|                text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|           hurt much|  1.0|       1.0|\n",
      "|teforia use machi...|  1.0|       1.0|\n",
      "|              finish|  1.0|       1.0|\n",
      "|future blase vice...|  1.0|       1.0|\n",
      "|              divine|  1.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"text\", \"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912655971479501"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 1.0]).build()\n",
    "# paramGrid = ParamGridBuilder().addGrid(rf.maxDepth,[4,8,10]).\\\n",
    "#                     addGrid(rf.impurity, ['entropy','gini']).build()\n",
    "\n",
    "\n",
    "# cv = CrossValidator(estimator=pipeline, \n",
    "#                     estimatorParamMaps=paramGrid, \n",
    "#                     evaluator=MulticlassClassificationEvaluator(), \n",
    "#                     numFolds=4)\n",
    "                    \n",
    "\n",
    "# #training_df.show(5)  \n",
    "# cvModel = cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction = cvModel.transform(test_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
